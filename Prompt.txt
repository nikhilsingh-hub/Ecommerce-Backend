I want to build a Proof of Concept E-commerce application. Please generate a complete, production-minded code scaffold and supporting artifacts in my preferred language (default: Node.js + TypeScript; alternative: Java Spring Boot — you can provide both if convenient). Keep best coding & design principles (SOLID, DI, separation of concerns, single responsibility, single source of truth, testability, observability, and extensibility) in mind.

Goal

A mini version of Elasticsearch + Kafka + application where:

MySQL holds the canonical product data.

Elasticsearch (ES) is used for search and related-products queries.

Kafka is replaced by a Publisher–Subscriber mechanism that:

Accepts batched message publishes (simulate producers writing batches).

Supports batch consumption by multiple parallel consumers (mimic Kafka consumer groups).

Demonstrates parallel processing, offsets/ack semantics (or an approximation), retries and idempotency.

Shows how this can be swapped to real Kafka later.

Functional requirements (APIs)

Provide REST APIs (and minimal GraphQL optional) with swagger/openapi:

POST /products — Add product (syncs to MySQL and publishes event to pub/sub).

GET /products/:id — Get product details.

PUT /products/:id — Update product (MySQL + event).

DELETE /products/:id — Delete product (MySQL + event).

GET /products/search?q=...&page=&size= — Search using ES with pagination & filters.

GET /products/:id/related — Return related/recommended products using ES (and fallback to MySQL logic).

POST /admin/publish-batch — Endpoint to publish a batch of events (for testing).

GET /health and metrics endpoint.

Non-functional & architecture requirements

Publisher–Subscriber:

Implement an in-process (or small service) pub/sub that supports batch publish and batch consume semantics.

Allow configuring number of parallel consumer workers and batch size.

Consumers should process messages in parallel and update ES (index/update/delete) or other downstream flows.

Show offset/ack simulation and simple checkpointing so reprocessing on failure is possible.

Consistency:

Use transactional patterns for MySQL writes + event emission (outbox pattern or simulate it), and eventual consistency for ES sync.

Explain and implement the outbox pattern (or pseudo-outbox) to avoid lost updates.

Scalability & resiliency:

Design so real Kafka/queue can replace the pub/sub easily.

Include retry/backoff, dead-letter handling for failed batches, idempotency keys.

Code quality:

Follow SOLID, dependency injection, layered architecture (controllers → services → repositories → integrations), DTOs and mappers.

Provide unit tests and integration tests for core flows (MySQL ↔ pub/sub ↔ ES).

Include README, setup scripts, Dockerfile(s), and a docker-compose.yml to run MySQL + ES + app locally.

Observability:

Add structured logging, basic metrics (request counts, processed batch counts, failures), and health checks.

Security & validation:

Input validation, error handling, and sensible defaults (no secrets in code).

Deliverables (what Cursor should generate)

Project skeleton with production-grade structure (src/, tests/, config/).

Implementation for chosen stack (Node.js + TS preferred) that runs locally via Docker Compose.

Pub/Sub module that simulates batched Kafka producers and parallel consumers (configurable).

MySQL schema and ES index mapping for products.

Outbox or transactional event pattern demo (comments + working example).

Sample scripts to publish test batches (sample data).

Postman/HTTP examples and OpenAPI docs.

Unit & integration tests (examples showing retries, idempotency, and batch processing).

README explaining architecture, how to run, how to swap pub/sub with Kafka later, and design decisions (SOLID examples).

(Optional) High-level diagram showing components and data flows.

Implementation details & expectations

Product model: id, name, description, categories, price, sku, attributes (json), images[], createdAt, updatedAt, popularity metrics (clicks, purchases).

ES mapping: support full-text search, keyword fields for faceting, nested attributes for filters.

Related products algorithm: primary: ES “more like this” / vector similarity (if possible); fallback: category + popularity heuristics.

Batch behavior:

Allow publishing N messages at once to the pub/sub.

Consumers should take batches up to configured batchSize, process in parallel worker threads/processes (or async workers), and ack batches.

Example scenario: bulk product import → application publishes batches → multiple consumer workers index into ES concurrently while updating metrics.

Testing scenarios to include in code/tests:

Successful batch processing.

Partial failure (one message causes error) and retry + DLQ.

Idempotent reprocessing (same event twice should not double-index).

Swap pub/sub to a stubbed Kafka client (show minimal adapter).

-----------------------------------------------------------------------------------